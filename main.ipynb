{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05d68f15-bdd9-464c-b83b-60c69a15a66c",
   "metadata": {},
   "source": [
    "## ğŸ§° Step 1. å¯¼å…¥ä¾èµ–ä¸è·¯å¾„è®¾ç½®\n",
    "æœ¬èŠ‚å¯¼å…¥æ‰€éœ€ä¾èµ–åŒ…ï¼Œå¹¶é…ç½®åŸºç¡€è·¯å¾„ã€‚\n",
    "- è‹¥ç¬¬ä¸€æ¬¡è¿è¡Œï¼Œéœ€è¦ç¡®ä¿ï¼š\n",
    "  - `lib/` ç›®å½•ä¸‹åŒ…å« `glove.840B.300d.txt`\n",
    "  - `corpus/imdb/` ç›®å½•ä¸‹åŒ…å« Kaggle æä¾›çš„ `.tsv` æ–‡ä»¶\n",
    "## ğŸ’¾ Step 2. åŠ è½½æˆ–ç”Ÿæˆ GloVe å‘é‡ï¼ˆ.kv æ ¼å¼ï¼‰\n",
    "æ­¤æ­¥éª¤ä¼šæ£€æµ‹ `lib/` ç›®å½•ä¸‹çš„æ–‡ä»¶ï¼š\n",
    "1. è‹¥å·²æœ‰ `glove.840B.300d.kv`ï¼Œåˆ™ç›´æ¥åŠ è½½ï¼›\n",
    "2. è‹¥åªæœ‰ `.gensim.txt` æˆ– `.txt` æ–‡ä»¶ï¼Œä¼šè‡ªåŠ¨è½¬æ¢ç”Ÿæˆ `.kv`ã€‚\n",
    "`.kv` æ ¼å¼çš„ä¼˜åŠ¿ï¼š\n",
    "- åŠ è½½æ›´å¿«ï¼ˆå‡ ç§’ï¼‰\n",
    "- å å†…å­˜æ›´å°‘ï¼ˆmmap æ–¹å¼ï¼‰\n",
    "- é¿å… `EOFError` åŠæ–‡ä»¶æŸåé£é™©\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "099c7bf7-73fe-45a8-91a8-bee9ea9535ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-19 13:15:35,603: INFO: å¼€å§‹æ£€æµ‹å¹¶åŠ è½½ GloVe æ–‡ä»¶...\n",
      "2025-10-19 13:15:35,608: INFO: æ£€æµ‹åˆ° .gensim.txt æ–‡ä»¶ï¼Œå°è¯•åŠ è½½: lib/glove.840B.300d.gensim.txt\n",
      "2025-10-19 13:15:35,609: INFO: loading projection weights from lib/glove.840B.300d.gensim.txt\n",
      "2025-10-19 13:20:30,023: WARNING: åŠ è½½ .gensim.txt å¤±è´¥: unexpected end of input; is count incorrect or file otherwise damaged?\n",
      "2025-10-19 13:20:30,260: WARNING: å°è¯•ä»åŸå§‹ glove.840B.300d.txt åŠ è½½ï¼ˆno_header=Trueï¼‰...\n",
      "2025-10-19 13:20:30,262: INFO: loading projection weights from lib/glove.840B.300d.txt\n",
      "2025-10-19 13:21:04,936: WARNING: duplicate word 'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½' in word2vec file, ignoring all but first\n",
      "2025-10-19 13:25:28,961: INFO: KeyedVectors lifecycle event {'msg': 'loaded (2196017, 300) matrix of type float32 from lib/glove.840B.300d.txt', 'binary': False, 'encoding': 'utf8', 'datetime': '2025-10-19T13:25:28.960865', 'gensim': '4.3.3', 'python': '3.12.3 | packaged by conda-forge | (main, Apr 15 2024, 18:20:11) [MSC v.1938 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26200-SP0', 'event': 'load_word2vec_format'}\n",
      "2025-10-19 13:25:28,963: INFO: åŠ è½½æˆåŠŸï¼ä¿å­˜ä¸º .kv ç¼“å­˜ä¸­...\n",
      "2025-10-19 13:25:28,967: INFO: KeyedVectors lifecycle event {'fname_or_handle': 'lib/glove.840B.300d.kv', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2025-10-19T13:25:28.967395', 'gensim': '4.3.3', 'python': '3.12.3 | packaged by conda-forge | (main, Apr 15 2024, 18:20:11) [MSC v.1938 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26200-SP0', 'event': 'saving'}\n",
      "2025-10-19 13:25:28,977: INFO: storing np array 'vectors' to lib/glove.840B.300d.kv.vectors.npy\n",
      "2025-10-19 13:25:33,015: INFO: saved lib/glove.840B.300d.kv\n",
      "2025-10-19 13:25:33,017: INFO: ä¿å­˜å®Œæˆï¼šlib/glove.840B.300d.kv\n",
      "2025-10-19 13:25:33,017: INFO: âœ… åŠ è½½å®Œæˆï¼šè¯æ±‡é‡ = 2196016ï¼Œç»´åº¦ = 300\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "glove_convert.py\n",
    "ç”¨é€”ï¼šå°† glove.840B.300d.txt è½¬æ¢ä¸ºé«˜æ•ˆçš„ gensim / kv æ ¼å¼\n",
    "æ”¯æŒè‡ªåŠ¨æ£€æµ‹æ–‡ä»¶ã€å¼‚å¸¸ä¿®å¤ã€æ—¥å¿—è¾“å‡º\n",
    "\"\"\"\n",
    "import os\n",
    "import logging\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# ========== æ—¥å¿—é…ç½® ==========\n",
    "logging.basicConfig(format=\"%(asctime)s: %(levelname)s: %(message)s\",\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ========== æ–‡ä»¶è·¯å¾„ ==========\n",
    "glove_txt = \"lib/glove.840B.300d.txt\"          # åŸå§‹å®˜æ–¹æ–‡ä»¶ï¼ˆ2GBï¼‰\n",
    "gensim_txt = \"lib/glove.840B.300d.gensim.txt\"  # è½¬æ¢åçš„æ–‡æœ¬\n",
    "kv_bin = \"lib/glove.840B.300d.kv\"              # é«˜æ•ˆäºŒè¿›åˆ¶ç¼“å­˜\n",
    "\n",
    "# ========== å¼€å§‹åŠ è½½ ==========\n",
    "logger.info(\"å¼€å§‹æ£€æµ‹å¹¶åŠ è½½ GloVe æ–‡ä»¶...\")\n",
    "\n",
    "wvmodel = None\n",
    "\n",
    "# 1ï¸âƒ£ ä¼˜å…ˆåŠ è½½äºŒè¿›åˆ¶ .kvï¼ˆæœ€å¿«ï¼‰\n",
    "if os.path.exists(kv_bin):\n",
    "    logger.info(\"æ£€æµ‹åˆ° .kv æ–‡ä»¶ï¼Œç›´æ¥åŠ è½½ï¼ˆmmap='r'ï¼‰: %s\", kv_bin)\n",
    "    wvmodel = KeyedVectors.load(kv_bin, mmap='r')\n",
    "\n",
    "# 2ï¸âƒ£ è‹¥æ—  .kvï¼Œå°è¯•åŠ è½½ .gensim.txt\n",
    "elif os.path.exists(gensim_txt):\n",
    "    try:\n",
    "        logger.info(\"æ£€æµ‹åˆ° .gensim.txt æ–‡ä»¶ï¼Œå°è¯•åŠ è½½: %s\", gensim_txt)\n",
    "        wvmodel = KeyedVectors.load_word2vec_format(gensim_txt, binary=False)\n",
    "        logger.info(\"åŠ è½½æˆåŠŸï¼Œä¿å­˜ä¸º .kv ä»¥ä¾¿ä¸‹æ¬¡å¿«é€Ÿè¯»å–...\")\n",
    "        wvmodel.save(kv_bin)\n",
    "        logger.info(\"å·²ä¿å­˜è‡³: %s\", kv_bin)\n",
    "    except Exception as e:\n",
    "        logger.warning(\"åŠ è½½ .gensim.txt å¤±è´¥: %s\", e)\n",
    "\n",
    "# 3ï¸âƒ£ è‹¥ä»¥ä¸Šéƒ½æ²¡æœ‰æˆ–æŸåï¼Œåˆ™ç›´æ¥ä» .txt è¯»å–ï¼ˆno_header=Trueï¼‰\n",
    "if wvmodel is None:\n",
    "    if not os.path.exists(glove_txt):\n",
    "        raise FileNotFoundError(\"æœªæ‰¾åˆ°åŸå§‹ GloVe æ–‡ä»¶ï¼Œè¯·ç¡®è®¤è·¯å¾„æ­£ç¡®ã€‚\")\n",
    "    logger.warning(\"å°è¯•ä»åŸå§‹ glove.840B.300d.txt åŠ è½½ï¼ˆno_header=Trueï¼‰...\")\n",
    "    wvmodel = KeyedVectors.load_word2vec_format(glove_txt, binary=False, no_header=True)\n",
    "    logger.info(\"åŠ è½½æˆåŠŸï¼ä¿å­˜ä¸º .kv ç¼“å­˜ä¸­...\")\n",
    "    wvmodel.save(kv_bin)\n",
    "    logger.info(\"ä¿å­˜å®Œæˆï¼š%s\", kv_bin)\n",
    "\n",
    "# ========== éªŒè¯åŠ è½½ç»“æœ ==========\n",
    "logger.info(\"âœ… åŠ è½½å®Œæˆï¼šè¯æ±‡é‡ = %dï¼Œç»´åº¦ = %d\",\n",
    "            len(wvmodel.key_to_index), wvmodel.vector_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6185b332-5a28-4ffa-9948-2af42e5cee3c",
   "metadata": {},
   "source": [
    "## ğŸ¬ Step 3. æ„å»º IMDB æ•°æ®é›†\n",
    "æ­¤æ­¥éª¤æ‰§è¡Œä»¥ä¸‹æµç¨‹ï¼š\n",
    "1. åŠ è½½ IMDB è®­ç»ƒ / æµ‹è¯• `.tsv` æ–‡ä»¶  \n",
    "2. ä½¿ç”¨ `BeautifulSoup` æ¸…æ´—æ–‡æœ¬  \n",
    "3. æ„å»ºè¯æ±‡è¡¨  \n",
    "4. ç¼–ç ä¸ºç´¢å¼•åºåˆ—å¹¶å¡«å……è‡³å›ºå®šé•¿åº¦  \n",
    "5. ä½¿ç”¨ GloVe å‘é‡æ„å»º embedding çŸ©é˜µ  \n",
    "6. åºåˆ—åŒ–è¾“å‡ºåˆ° `pickle/imdb_glove.pickle3`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aaff8dec-4db7-46dd-8a03-77f0f752fa61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-19 13:43:08,923: INFO: running D:\\Anaconda\\Lib\\site-packages\\ipykernel_launcher.py -f C:\\Users\\22711\\AppData\\Roaming\\jupyter\\runtime\\kernel-a488ede4-750d-4260-805b-7db9d5cf9eb4.json\n",
      "2025-10-19 13:43:09,673: INFO: train shape: (25000, 3) | test shape: (25000, 2)\n",
      "2025-10-19 13:43:09,674: INFO: cleaning & tokenizing train/test reviews ...\n",
      "C:\\Users\\22711\\AppData\\Local\\Temp\\ipykernel_17584\\567924533.py:42: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  review_text = BeautifulSoup(str(review), \"html.parser\").get_text()\n",
      "2025-10-19 13:43:24,728: INFO: building vocabulary ...\n",
      "2025-10-19 13:43:25,303: INFO: vocab size: 101399\n",
      "2025-10-19 13:43:25,303: INFO: train/val split ...\n",
      "2025-10-19 13:43:25,342: INFO: loading GloVe model from lib\\glove.840B.300d.kv\n",
      "2025-10-19 13:43:25,343: INFO: loading KeyedVectors object from lib\\glove.840B.300d.kv\n",
      "2025-10-19 13:43:26,273: INFO: loading vectors from lib\\glove.840B.300d.kv.vectors.npy with mmap=r\n",
      "2025-10-19 13:43:26,288: INFO: KeyedVectors lifecycle event {'fname': 'lib\\\\glove.840B.300d.kv', 'datetime': '2025-10-19T13:43:26.288106', 'gensim': '4.3.3', 'python': '3.12.3 | packaged by conda-forge | (main, Apr 15 2024, 18:20:11) [MSC v.1938 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26200-SP0', 'event': 'loaded'}\n",
      "2025-10-19 13:43:26,289: INFO: GloVe loaded: vocab=2196016, dim=300\n",
      "2025-10-19 13:43:26,289: INFO: building word_to_idx / idx_to_word ...\n",
      "2025-10-19 13:43:26,354: INFO: encoding & padding ...\n",
      "2025-10-19 13:43:31,703: INFO: building embedding weight matrix ...\n",
      "C:\\Users\\22711\\AppData\\Local\\Temp\\ipykernel_17584\\567924533.py:149: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_numpy.cpp:212.)\n",
      "  weight[idx, :] = torch.from_numpy(wvmodel.get_vector(word))\n",
      "2025-10-19 13:43:43,663: INFO: initialized embeddings for 77553 / 101399 words (76.48% covered)\n",
      "2025-10-19 13:43:43,664: INFO: saving dataset to pickle/imdb_glove.pickle3 ...\n",
      "2025-10-19 13:43:44,216: INFO: âœ… Done! Saved preprocessed data to pickle/imdb_glove.pickle3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "imdb_glove.py\n",
    "åŠŸèƒ½ï¼šå°† IMDB å½±è¯„æ•°æ®é›†è½¬æ¢ä¸ºåŸºäº GloVe é¢„è®­ç»ƒè¯å‘é‡çš„ç´¢å¼•åŒ–å¼ é‡æ•°æ®\n",
    "è¾“å‡ºï¼špickle/imdb_glove.pickle3\n",
    "\"\"\"\n",
    "\n",
    "import csv\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from itertools import chain\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ========= åŸºæœ¬å‚æ•° =========\n",
    "embed_size = 300         # GloVe ç»´åº¦\n",
    "max_len = 512            # æ¯æ¡æ ·æœ¬çš„æœ€å¤§é•¿åº¦\n",
    "imdb_dir = \"./corpus/imdb\"\n",
    "pickle_out = \"pickle/imdb_glove.pickle3\"\n",
    "\n",
    "# ========= æ•°æ®è·¯å¾„ =========\n",
    "train_path = os.path.join(imdb_dir, \"labeledTrainData.tsv\")\n",
    "test_path = os.path.join(imdb_dir, \"testData.tsv\")\n",
    "unlabeled_path = os.path.join(imdb_dir, \"unlabeledTrainData.tsv\")  # å¯é€‰æ–‡ä»¶\n",
    "\n",
    "# ========= GloVe è·¯å¾„ =========\n",
    "kv_file = os.path.join(\"lib\", \"glove.840B.300d.kv\")  # å·²ç”Ÿæˆçš„é«˜æ•ˆæ–‡ä»¶\n",
    "\n",
    "\n",
    "# ========== æ–‡æœ¬æ¸…æ´—å‡½æ•° ==========\n",
    "def review_to_wordlist(review, remove_stopwords=False):\n",
    "    \"\"\"\n",
    "    æ–‡æœ¬æ¸…æ´—ï¼šå» HTML æ ‡ç­¾ã€å»éå­—æ¯å­—ç¬¦ã€å°å†™åŒ–å¹¶åˆ†è¯\n",
    "    \"\"\"\n",
    "    review_text = BeautifulSoup(str(review), \"html.parser\").get_text()\n",
    "    review_text = re.sub(r\"[^a-zA-Z]\", \" \", review_text)\n",
    "    words = review_text.lower().split()\n",
    "    return words\n",
    "\n",
    "\n",
    "# ========== ç¼–ç å‡½æ•° ==========\n",
    "def encode_samples(tokenized_samples, word_to_idx):\n",
    "    \"\"\"\n",
    "    å°†è¯è½¬æ¢ä¸ºç´¢å¼•åºåˆ—ï¼ˆæœªçŸ¥è¯ä¸º 0ï¼‰\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    for sample in tokenized_samples:\n",
    "        feature = [word_to_idx.get(token, 0) for token in sample]\n",
    "        features.append(feature)\n",
    "    return features\n",
    "\n",
    "\n",
    "# ========== Padding å‡½æ•° ==========\n",
    "def pad_samples(features, maxlen=max_len, PAD=0):\n",
    "    \"\"\"\n",
    "    æˆªæ–­æˆ–å¡«å……æ ·æœ¬åˆ°å›ºå®šé•¿åº¦\n",
    "    \"\"\"\n",
    "    padded = []\n",
    "    for seq in features:\n",
    "        if len(seq) >= maxlen:\n",
    "            padded.append(seq[:maxlen])\n",
    "        else:\n",
    "            padded.append(seq + [PAD] * (maxlen - len(seq)))\n",
    "    return padded\n",
    "\n",
    "\n",
    "# ========== ä¸»ç¨‹åº ==========\n",
    "def main():\n",
    "    # ========== æ—¥å¿—é…ç½® ==========\n",
    "    program = os.path.basename(sys.argv[0])\n",
    "    logger = logging.getLogger(program)\n",
    "    logging.basicConfig(format=\"%(asctime)s: %(levelname)s: %(message)s\")\n",
    "    logging.root.setLevel(level=logging.INFO)\n",
    "    logger.info(\"running %s\", \" \".join(sys.argv))\n",
    "\n",
    "    # ========== è·¯å¾„æ£€æŸ¥ ==========\n",
    "    for p in [train_path, test_path]:\n",
    "        if not os.path.exists(p):\n",
    "            raise FileNotFoundError(f\"æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶ï¼š{p}ï¼Œè¯·ç¡®è®¤ IMDB TSV å·²æ”¾åœ¨ {imdb_dir}\")\n",
    "\n",
    "    if not os.path.exists(kv_file):\n",
    "        raise FileNotFoundError(f\"æœªæ‰¾åˆ° {kv_file}ï¼Œè¯·å…ˆè¿è¡Œ glove_convert.py ç”Ÿæˆ .kv æ–‡ä»¶\")\n",
    "\n",
    "    os.makedirs(os.path.dirname(pickle_out), exist_ok=True)\n",
    "\n",
    "    # ========== è¯»å– IMDB æ•°æ® ==========\n",
    "    train = pd.read_csv(train_path, header=0, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    test = pd.read_csv(test_path, header=0, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    logger.info(\"train shape: %s | test shape: %s\", train.shape, test.shape)\n",
    "\n",
    "    # ========== æ–‡æœ¬æ¸…æ´— ==========\n",
    "    logger.info(\"cleaning & tokenizing train/test reviews ...\")\n",
    "    clean_train_reviews, train_labels = [], []\n",
    "    for i, review in enumerate(train[\"review\"]):\n",
    "        clean_train_reviews.append(review_to_wordlist(review))\n",
    "        train_labels.append(int(train[\"sentiment\"][i]))\n",
    "    clean_test_reviews = [review_to_wordlist(r) for r in test[\"review\"]]\n",
    "\n",
    "    # ========== æ„å»ºè¯è¡¨ ==========\n",
    "    logger.info(\"building vocabulary ...\")\n",
    "    vocab = set(chain(*clean_train_reviews)) | set(chain(*clean_test_reviews))\n",
    "    vocab_size = len(vocab)\n",
    "    logger.info(\"vocab size: %d\", vocab_size)\n",
    "\n",
    "    # ========== åˆ’åˆ†éªŒè¯é›† ==========\n",
    "    logger.info(\"train/val split ...\")\n",
    "    train_reviews, val_reviews, train_labels_arr, val_labels_arr = train_test_split(\n",
    "        clean_train_reviews, train_labels, test_size=0.2, random_state=0\n",
    "    )\n",
    "\n",
    "    # ========== åŠ è½½ GloVe ==========\n",
    "    logger.info(\"loading GloVe model from %s\", kv_file)\n",
    "    wvmodel = KeyedVectors.load(kv_file, mmap='r')\n",
    "    logger.info(\"GloVe loaded: vocab=%d, dim=%d\",\n",
    "                len(wvmodel.key_to_index), wvmodel.vector_size)\n",
    "\n",
    "    # ========== æ„å»ºè¯ â†” ç´¢å¼• ==========\n",
    "    logger.info(\"building word_to_idx / idx_to_word ...\")\n",
    "    word_to_idx = {word: i + 1 for i, word in enumerate(vocab)}\n",
    "    word_to_idx[\"<unk>\"] = 0\n",
    "    idx_to_word = {i + 1: word for i, word in enumerate(vocab)}\n",
    "    idx_to_word[0] = \"<unk>\"\n",
    "\n",
    "    # ========== ç¼–ç  & padding ==========\n",
    "    logger.info(\"encoding & padding ...\")\n",
    "    train_features = torch.tensor(pad_samples(encode_samples(train_reviews, word_to_idx)))\n",
    "    val_features = torch.tensor(pad_samples(encode_samples(val_reviews, word_to_idx)))\n",
    "    test_features = torch.tensor(pad_samples(encode_samples(clean_test_reviews, word_to_idx)))\n",
    "\n",
    "    train_labels_t = torch.tensor(train_labels_arr, dtype=torch.long)\n",
    "    val_labels_t = torch.tensor(val_labels_arr, dtype=torch.long)\n",
    "\n",
    "    # ========== æ„å»º embedding æƒé‡çŸ©é˜µ ==========\n",
    "    logger.info(\"building embedding weight matrix ...\")\n",
    "    weight = torch.zeros(vocab_size + 1, embed_size, dtype=torch.float32)\n",
    "\n",
    "    exist = 0\n",
    "    for word, idx in word_to_idx.items():\n",
    "        if word == \"<unk>\":\n",
    "            continue\n",
    "        if word in wvmodel.key_to_index:\n",
    "            weight[idx, :] = torch.from_numpy(wvmodel.get_vector(word))\n",
    "            exist += 1\n",
    "    logger.info(\"initialized embeddings for %d / %d words (%.2f%% covered)\",\n",
    "                exist, vocab_size, 100 * exist / vocab_size)\n",
    "\n",
    "    # ========== ä¿å­˜ ==========\n",
    "    logger.info(\"saving dataset to %s ...\", pickle_out)\n",
    "    with open(pickle_out, \"wb\") as f:\n",
    "        pickle.dump(\n",
    "            [\n",
    "                train_features,\n",
    "                train_labels_t,\n",
    "                val_features,\n",
    "                val_labels_t,\n",
    "                test_features,\n",
    "                weight,\n",
    "                word_to_idx,\n",
    "                idx_to_word,\n",
    "                vocab,\n",
    "            ],\n",
    "            f,\n",
    "        )\n",
    "    logger.info(\"âœ… Done! Saved preprocessed data to %s\", pickle_out)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e90d04-8d93-4fd9-a63b-2d5bb8263979",
   "metadata": {},
   "source": [
    "## ğŸ” Step 4. éªŒè¯æ•°æ®æ–‡ä»¶\n",
    "åŠ è½½å¹¶æŸ¥çœ‹ç»“æ„æ˜¯å¦æ­£ç¡®ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5029607-74d5-4f9e-aa5d-4cbd79ae4b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ•°æ®æ–‡ä»¶åŠ è½½æˆåŠŸï¼\n",
      "\n",
      "è®­ç»ƒé›†ç‰¹å¾: torch.Size([20000, 512]), æ ‡ç­¾: torch.Size([20000])\n",
      "éªŒè¯é›†ç‰¹å¾: torch.Size([5000, 512]), æ ‡ç­¾: torch.Size([5000])\n",
      "æµ‹è¯•é›†ç‰¹å¾: torch.Size([25000, 512])\n",
      "Embedding çŸ©é˜µ: torch.Size([101400, 300])\n",
      "è¯æ±‡è¡¨å¤§å°: 101399\n",
      "ç¤ºä¾‹è¯å‘é‡ï¼ˆ'good'ï¼‰:\n",
      "tensor([-0.4263,  0.4431, -0.3452, -0.1326, -0.0582,  0.0526,  0.2157, -0.3672,\n",
      "        -0.0452,  2.2444])\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "check_imdb_pickle.py\n",
    "å¿«é€Ÿæ£€æŸ¥ imdb_glove.pickle3 æ˜¯å¦æ­£ç¡®ç”Ÿæˆ\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "path = \"pickle/imdb_glove.pickle3\"\n",
    "\n",
    "with open(path, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "train_features, train_labels, val_features, val_labels, test_features, weight, word_to_idx, idx_to_word, vocab = data\n",
    "\n",
    "print(\"âœ… æ•°æ®æ–‡ä»¶åŠ è½½æˆåŠŸï¼\\n\")\n",
    "print(f\"è®­ç»ƒé›†ç‰¹å¾: {train_features.shape}, æ ‡ç­¾: {train_labels.shape}\")\n",
    "print(f\"éªŒè¯é›†ç‰¹å¾: {val_features.shape}, æ ‡ç­¾: {val_labels.shape}\")\n",
    "print(f\"æµ‹è¯•é›†ç‰¹å¾: {test_features.shape}\")\n",
    "print(f\"Embedding çŸ©é˜µ: {weight.shape}\")\n",
    "print(f\"è¯æ±‡è¡¨å¤§å°: {len(vocab)}\")\n",
    "print(f\"ç¤ºä¾‹è¯å‘é‡ï¼ˆ'good'ï¼‰:\")\n",
    "print(weight[word_to_idx.get('good', 0)][:10])  # æ‰“å°å‰10ä¸ªç»´åº¦\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85352fc1-d626-4bce-8583-925aad26d64b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
